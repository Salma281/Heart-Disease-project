{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc11b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Logistic Regression...\n",
      "Best parameters: {'C': 1, 'solver': 'lbfgs'}\n",
      "Best CV score: 0.8511054421768707\n",
      "Test set performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89        33\n",
      "           1       0.84      0.93      0.88        28\n",
      "\n",
      "    accuracy                           0.89        61\n",
      "   macro avg       0.89      0.89      0.89        61\n",
      "weighted avg       0.89      0.89      0.89        61\n",
      "\n",
      "\n",
      "Tuning Decision Tree...\n",
      "Best parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Best CV score: 0.7850340136054421\n",
      "Test set performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84        33\n",
      "           1       0.81      0.79      0.80        28\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.82      0.82      0.82        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n",
      "\n",
      "Tuning Random Forest...\n",
      "Best parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best CV score: 0.8221938775510204\n",
      "Test set performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92        33\n",
      "           1       0.87      0.96      0.92        28\n",
      "\n",
      "    accuracy                           0.92        61\n",
      "   macro avg       0.92      0.92      0.92        61\n",
      "weighted avg       0.92      0.92      0.92        61\n",
      "\n",
      "\n",
      "Tuning SVM...\n",
      "Best parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best CV score: 0.8388605442176871\n",
      "Test set performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        33\n",
      "           1       0.81      0.89      0.85        28\n",
      "\n",
      "    accuracy                           0.85        61\n",
      "   macro avg       0.85      0.86      0.85        61\n",
      "weighted avg       0.86      0.85      0.85        61\n",
      "\n",
      "\n",
      "Final best model saved as final_model.pkl (Random Forest)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv(\"../data/selected_features.csv\")\n",
    "X = df.drop(\"num\", axis=1)\n",
    "y = (df[\"num\"] > 0).astype(int)  # binary target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Define models & parameter grids\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],\n",
    "        \"solver\": [\"liblinear\", \"lbfgs\"]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": [3, 5, 7, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": [\"linear\", \"rbf\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "# 3. Run GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "    print(\"Best CV score:\", grid.best_score_)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "    print(\"Test set performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    best_models[name] = grid.best_estimator_\n",
    "\n",
    "# Select the model with the highest test accuracy\n",
    "best_model_name = max(best_models, key=lambda k: best_models[k].score(X_test, y_test))\n",
    "final_model = best_models[best_model_name]\n",
    "\n",
    "joblib.dump(final_model, \"../models/final_model.pkl\")\n",
    "print(f\"\\nFinal best model saved as final_model.pkl ({best_model_name})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
